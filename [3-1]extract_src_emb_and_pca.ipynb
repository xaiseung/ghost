{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe33e348",
   "metadata": {},
   "source": [
    "# 코드설명(임시)\n",
    "\n",
    "- source embedding 추출하는 코드\n",
    "- source pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "published-metro",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/kornia/augmentation/augmentation.py:1830: DeprecationWarning: GaussianBlur is no longer maintained and will be removed from the future versions. Please use RandomGaussianBlur instead.\n",
      "  warnings.warn(\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:143: DeprecationWarning: In accordance with NEP 32, the function mirr was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  mirr = onp.mirr\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:160: DeprecationWarning: In accordance with NEP 32, the function npv was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  npv = onp.npv\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:164: DeprecationWarning: In accordance with NEP 32, the function pmt was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  pmt = onp.pmt\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:173: DeprecationWarning: In accordance with NEP 32, the function ppmt was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  ppmt = onp.ppmt\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:176: DeprecationWarning: In accordance with NEP 32, the function pv was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  pv = onp.pv\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:177: DeprecationWarning: In accordance with NEP 32, the function rate was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  rate = onp.rate\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy_dispatch_protocol.py:48: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  cur_np_ver = LooseVersion(_np.__version__)\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy_dispatch_protocol.py:49: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  np_1_17_ver = LooseVersion('1.17')\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy_dispatch_protocol.py:68: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  cur_np_ver = LooseVersion(_np.__version__)\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy_dispatch_protocol.py:69: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  np_1_15_ver = LooseVersion('1.15')\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "def nop(it, *a, **k):\n",
    "    return it\n",
    "\n",
    "real_tqdm = tqdm.tqdm\n",
    "tqdm.tqdm = nop\n",
    "\n",
    "import pickle\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from utils.inference.faceshifter_run import faceshifter_batch\n",
    "from utils.inference.image_processing import crop_face, get_final_image, show_images, normalize_and_torch, normalize_and_torch_batch\n",
    "from utils.inference.video_processing import read_video, get_target, get_final_video, add_audio_from_another_video, face_enhancement, crop_frames_and_get_transforms, resize_frames\n",
    "from utils.inference.core import model_inference, transform_target_to_torch\n",
    "\n",
    "from network.AEI_Net import AEI_Net\n",
    "from coordinate_reg.image_infer import Handler\n",
    "from insightface_func.face_detect_crop_multi import Face_detect_crop\n",
    "from arcface_model.iresnet import iresnet100\n",
    "from models.pix2pix_model import Pix2PixModel\n",
    "from models.config_sr import TestOptions\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-factor",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "signed-mustang",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input mean and std: 127.5 127.5\n",
      "find model: ./insightface_func/models/antelope/glintr100.onnx recognition\n",
      "find model: ./insightface_func/models/antelope/scrfd_10g_bnkps.onnx detection\n",
      "set det-size: (640, 640)\n",
      "loading ./coordinate_reg/model/2d106det 0\n",
      "input mean and std: 127.5 127.5\n",
      "find model: ./insightface_func/models/antelope/glintr100.onnx recognition\n",
      "find model: ./insightface_func/models/antelope/scrfd_10g_bnkps.onnx detection\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:09:40] ../src/nnvm/legacy_json_util.cc:208: Loading symbol saved by previous version v1.5.0. Attempting to upgrade...\n",
      "[10:09:40] ../src/nnvm/legacy_json_util.cc:216: Symbol successfully upgraded!\n",
      "[10:09:40] ../src/base.cc:79: cuDNN lib mismatch: linked-against version 8204 != compiled-against version 8101.  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network [LIPSPADEGenerator] was created. Total number of parameters: 72.2 million. To see the architecture, do print(network).\n",
      "Load checkpoint from path:  weights/10_net_G.pth\n"
     ]
    }
   ],
   "source": [
    "app = Face_detect_crop(name='antelope', root='./insightface_func/models')\n",
    "app.prepare(ctx_id= 0, det_thresh=0.6, det_size=(640,640))\n",
    "\n",
    "# main model for generation\n",
    "G = AEI_Net(backbone='unet', num_blocks=2, c_id=512)\n",
    "G.eval()\n",
    "G.load_state_dict(torch.load('weights/G_unet_2blocks.pth', map_location=torch.device('cpu')))\n",
    "G = G.cuda()\n",
    "G = G.half()\n",
    "\n",
    "# arcface model to get face embedding\n",
    "netArc = iresnet100(fp16=False)\n",
    "netArc.load_state_dict(torch.load('arcface_model/backbone.pth'))\n",
    "netArc=netArc.cuda()\n",
    "netArc.eval()\n",
    "\n",
    "# model to get face landmarks\n",
    "handler = Handler('./coordinate_reg/model/2d106det', 0, ctx_id=0, det_size=640)\n",
    "\n",
    "# model to make superres of face, set use_sr=True if you want to use super resolution or use_sr=False if you don't\n",
    "use_sr = True\n",
    "if use_sr:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    opt = TestOptions()\n",
    "    #opt.which_epoch ='10_7'\n",
    "    model = Pix2PixModel(opt)\n",
    "    model.netG.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d613e79",
   "metadata": {},
   "source": [
    "# Source embeding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 224\n",
    "np.random.seed(7777)\n",
    "\n",
    "id_folder_list = glob.glob(\"../vggFace2_Train/*\")\n",
    "id_folder_list.sort()\n",
    "with open(\"embed_list.txt\", \"w\") as embed_log:\n",
    "    output_cnt = 0\n",
    "    for id_folder in real_tqdm(id_folder_list):\n",
    "        pics = glob.glob(id_folder+\"/*.jpg\")\n",
    "        pics.sort()\n",
    "        for pic in pics: #pics[:5]:\n",
    "            source_full = cv2.imread(pic)\n",
    "            source_curr = cv2.resize(source_full[:, :, ::-1], (crop_size, crop_size))\n",
    "\n",
    "            source_curr = normalize_and_torch(source_curr)\n",
    "            source_embed = netArc(F.interpolate(source_curr, scale_factor=0.5, mode='bilinear', align_corners=True)).cpu().detach().numpy()\n",
    "            \n",
    "            np.save(\"./examples/arc_embeds/\"+str(output_cnt)+\".npy\", source_embed[0])\n",
    "            embed_log.write(pic+\"\\n\")\n",
    "            output_cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66534486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(svd_solver=&#x27;full&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;PCA<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.decomposition.PCA.html\">?<span>Documentation for PCA</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>PCA(svd_solver=&#x27;full&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "PCA(svd_solver='full')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "embeds_path_list = glob.glob(\"./examples/arc_embeds/*.npy\")\n",
    "embeds_path_list.sort()\n",
    "\n",
    "tmp = np.load(embeds_path_list[0])\n",
    "\n",
    "embed_array = np.zeros([len(embeds_path_list), tmp.shape[0]], dtype=tmp.dtype)\n",
    "\n",
    "for i, embed_path in enumerate(embeds_path_list):\n",
    "    embed_array[i] = np.load(embed_path)\n",
    "\n",
    "pca = PCA(svd_solver=\"full\")\n",
    "\n",
    "pca.fit(embed_array)\n",
    "import pickle\n",
    "with open(\"netArcPCA.pkl\", \"wb\") as file:\n",
    "    pickle.dump(pca, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "60480a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.45831309 -2.91218561 -2.74709605 ... -0.0109711   0.06906028\n",
      "  -0.01949976]\n",
      " [-1.54323984 -2.88345135 -2.56226107 ...  0.01646118  0.0200644\n",
      "   0.03217148]\n",
      " [-1.50888864 -2.91336455 -2.63369709 ... -0.02764956  0.00526128\n",
      "   0.00727385]\n",
      " ...\n",
      " [-1.46263548 -2.88010943 -2.65175549 ... -0.00343136 -0.02023641\n",
      "  -0.00704339]\n",
      " [-1.53983292 -2.88415606 -2.68146959 ...  0.01861276  0.0177595\n",
      "  -0.03548871]\n",
      " [-1.43163991 -2.90456182 -2.58674712 ...  0.00542292 -0.00335126\n",
      "   0.02753525]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"netArcPCA.pkl\", \"wb\") as file:\n",
    "    pickle.dump(pca, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa709fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "with open(\"netArcPCA.pkl\", \"rb\") as file:\n",
    "    pca = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2283e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_array = pca.transform(embed_array)\n",
    "\n",
    "pca_minmax = dict({\"min\": np.amin(pca_array, axis=0), \"max\": np.amax(pca_array, axis=0)})\n",
    "\n",
    "with open(\"netArcPCAMinMax.pkl\", \"wb\") as file:\n",
    "    pickle.dump(pca_minmax, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a05a86e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.73729     6.45828     1.1065898   1.4121022  -0.36449677 -1.1651448\n",
      "  0.34845555  2.0992432   0.9984462   0.3670262 ]\n",
      "[ 0.73729     6.4582796   1.10659     1.4121022  -0.3644966  -1.1651448\n",
      "  0.34845543  2.0992432   0.99844635  0.36702624]\n"
     ]
    }
   ],
   "source": [
    "print(pca_array[0][:10])\n",
    "print((embed_array[0]-embed_array.mean(axis=0)).dot(pca.components_.T)[:10])\n",
    "#pca.components_[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1b3ffd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 3086.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.inference.core import transform_target_to_torch\n",
    "\n",
    "#frame = cv2.imread(\"../vggFace2_Train/n000002/0002_01.jpg\")\n",
    "\n",
    "#crop_frames_list = [[cv2.resize(frame, (crop_size, crop_size))]]\n",
    "\n",
    "print(crop_frames_list[0][0].shape)\n",
    "plt.imshow(crop_frames_list[0][0])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "resized_frs, present = resize_frames(crop_frames_list[0])\n",
    "resized_frs = np.array(resized_frs)\n",
    "\n",
    "target_batch_rs = transform_target_to_torch(resized_frs, half=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "source_full = cv2.imread(pic)\n",
    "#print(source_full.shape)\n",
    "#source = crop_face(source_full, app, crop_size)[0]\n",
    "source_curr = cv2.resize(source_full[:, :, ::-1], (crop_size, crop_size))\n",
    "source_curr = normalize_and_torch(source_curr)\n",
    "#print(source_curr.shape)\n",
    "source_embed = netArc(F.interpolate(source_curr, scale_factor=0.5, mode='bilinear', align_corners=True))\n",
    "source_embed = source_embed.half()\n",
    "a\n",
    "\"\"\"\n",
    "id = 1\n",
    "\n",
    "modified_embed = pca.inverse_transform(pca_array[id:id+1])\n",
    "\n",
    "source_embed = torch.from_numpy(modified_embed).half().to(\"cuda\")\n",
    "#source_embed[0][0] = 10.0\n",
    "\n",
    "Y_st = faceshifter_batch(source_embed, target_batch_rs, G)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "plt.imshow(Y_st[0][:, :, ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ad5c5d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.inference.faceshifter_run import faceshifter_batch\n",
    "from utils.inference.image_processing import crop_face, normalize_and_torch, normalize_and_torch_batch\n",
    "from utils.inference.video_processing import read_video, crop_frames_and_get_transforms, resize_frames\n",
    "from utils.inference.core import transform_target_to_torch\n",
    "\n",
    "set_target = False\n",
    "half=True\n",
    "similarity_th=0.15\n",
    "\n",
    "\n",
    "target_norm = normalize_and_torch_batch(np.array(target))\n",
    "target_embeds = netArc(F.interpolate(target_norm, scale_factor=0.5, mode='bilinear', align_corners=True))\n",
    "\n",
    "# Get the cropped faces from original frames and transformations to get those crops\n",
    "crop_frames_list, tfm_array_list = crop_frames_and_get_transforms(full_frames, target_embeds, app, netArc, crop_size, set_target, similarity_th=similarity_th)\n",
    "\n",
    "# Normalize source images and transform to torch and get Arcface embeddings\n",
    "source_embeds = []\n",
    "for source_curr in source:\n",
    "    source_curr = normalize_and_torch(source_curr)\n",
    "    source_embeds.append(netArc(F.interpolate(source_curr, scale_factor=0.5, mode='bilinear', align_corners=True)))\n",
    "\n",
    "print(source_embeds[0].shape)\n",
    "\n",
    "print(crop_frames_list[0][0].shape, crop_frames_list[0][0].dtype)\n",
    "\n",
    "plt.imshow(crop_frames_list[0][0])\n",
    "plt.show()\n",
    "\n",
    "final_frames_list = []\n",
    "for idx, (crop_frames, tfm_array, source_embed) in enumerate(zip(crop_frames_list, tfm_array_list, source_embeds)):\n",
    "    # Resize croped frames and get vector which shows on which frames there were faces\n",
    "    resized_frs, present = resize_frames(crop_frames)\n",
    "    resized_frs = np.array(resized_frs)\n",
    "\n",
    "    # transform embeds of Xs and target frames to use by model\n",
    "    target_batch_rs = transform_target_to_torch(resized_frs, half=half)\n",
    "    print(target_batch_rs.shape)\n",
    "    #assert False\n",
    "    if half:\n",
    "        source_embed = source_embed.half()\n",
    "\n",
    "    # run model\n",
    "    size = target_batch_rs.shape[0]\n",
    "    model_output = []\n",
    "\n",
    "    for i in tqdm(range(0, size, BS)):\n",
    "        Y_st = faceshifter_batch(source_embed, target_batch_rs[i:i+BS], G)\n",
    "        model_output.append(Y_st)\n",
    "    torch.cuda.empty_cache()\n",
    "    model_output = np.concatenate(model_output)\n",
    "\n",
    "    # create list of final frames with transformed faces\n",
    "    final_frames = []\n",
    "    idx_fs = 0\n",
    "\n",
    "    for pres in tqdm(present):\n",
    "        if pres == 1:\n",
    "            final_frames.append(model_output[idx_fs])\n",
    "            idx_fs += 1\n",
    "        else:\n",
    "            final_frames.append([])\n",
    "    final_frames_list.append(final_frames)\n",
    "    assert False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba80fc04",
   "metadata": {},
   "source": [
    "# target 중간값 특징 추출\n",
    "- 쓰면 컴퓨터 멸망함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b2d876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "torch.Size([1, 512])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;66;03m#assert False\u001b[39;00m\n\u001b[1;32m     75\u001b[0m             output_cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 76\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03mset_target = False\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mhalf=True\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    assert False\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.inference.faceshifter_run import faceshifter_batch\n",
    "from utils.inference.image_processing import crop_face, normalize_and_torch, normalize_and_torch_batch\n",
    "from utils.inference.video_processing import read_video, crop_frames_and_get_transforms, resize_frames\n",
    "from utils.inference.core import transform_target_to_torch\n",
    "\n",
    "\n",
    "import time\n",
    "\"\"\"\n",
    "set_target = False\n",
    "half=True\n",
    "similarity_th=0.15\n",
    "crop_size=224\n",
    "np.random.seed(7777)\n",
    "\n",
    "id_folder_list = glob.glob(\"./examples/images/VggFace2-crop/*\")\n",
    "id_folder_list.sort()\n",
    "with open(\"VggFace2-cropzattr_embed_list.txt\", \"w\") as embed_log:\n",
    "    output_cnt = 0\n",
    "    for id_folder in real_tqdm(id_folder_list):\n",
    "        pics = glob.glob(id_folder+\"/*.jpg\")\n",
    "        pics.sort()\n",
    "        np.random.shuffle(pics)\n",
    "        for pic in pics[:2]:\n",
    "            source_full = cv2.imread(pic)\n",
    "            full_frames = [source_full]\n",
    "            #print(source_full.shape)\n",
    "            #source = crop_face(source_full, app, crop_size)[0]\n",
    "            source_curr = cv2.resize(source_full[:, :, ::-1], (crop_size, crop_size))\n",
    "            target = [source_curr.copy()]\n",
    "            source_curr = normalize_and_torch(source_curr)\n",
    "            #print(source_curr.shape)\n",
    "            source_embed = netArc(F.interpolate(source_curr, scale_factor=0.5, mode='bilinear', align_corners=True))\n",
    "\n",
    "\n",
    "            #get_target(full_frames, app, crop_size)\n",
    "            print(target[0].shape)\n",
    "            target_norm = normalize_and_torch_batch(np.array(target))\n",
    "\n",
    "            target_embeds = netArc(F.interpolate(target_norm, scale_factor=0.5, mode='bilinear', align_corners=True))\n",
    "\n",
    "            # Get the cropped faces from original frames and transformations to get those crops\n",
    "            #crop_frames_list, tfm_array_list = crop_frames_and_get_transforms(full_frames, target_embeds, app, netArc, crop_size, set_target, similarity_th=similarity_th)\n",
    "            #crop_frames = crop_frames_list[0]\n",
    "\n",
    "            crop_frames = target\n",
    "            resized_frs, present = resize_frames(crop_frames)\n",
    "            resized_frs = np.array(resized_frs)\n",
    "\n",
    "\n",
    "            target_batch_rs = transform_target_to_torch(resized_frs, half=half)\n",
    "            #assert False\n",
    "            if half:\n",
    "                source_embed = source_embed.half()\n",
    "            \n",
    "            bs = target_batch_rs.shape[0]\n",
    "            source_emb = torch.cat([source_embed]*bs)\n",
    "            \n",
    "            print(source_emb.shape)\n",
    "            #Y_st = faceshifter_batch(source_embed, target_batch_rs, G)\n",
    "            Y_st, zattrs = G(target_batch_rs, source_emb)\n",
    "            zattrs = list(zattrs)\n",
    "            for i in range(len(zattrs)):\n",
    "                zattrs[i] = zattrs[i].detach().cpu().numpy()\n",
    "            #print(source_embed.shape)\n",
    "\n",
    "            with open(\"./examples/z_embeds/VggFace2-crop/\"+str(output_cnt)+\".pkl\", \"wb\") as file:\n",
    "                pickle.dump(zattrs, file)\n",
    "            embed_log.write(pic+\"\\n\")\n",
    "            #assert False\n",
    "            output_cnt += 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95af4ce",
   "metadata": {},
   "source": [
    "# PCA for zattr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ca684e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "emb_array = []\n",
    "\n",
    "emb_file_list = glob.glob(\"./examples/z_embeds/VggFace2-crop/*.pkl\")\n",
    "\n",
    "for file_path in emb_file_list:\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        emb_array.append(pickle.load(file)[0].reshape([-1]))\n",
    "emb_array = np.array(emb_array)\n",
    "\n",
    "\n",
    "pca = PCA(svd_solver=\"full\")\n",
    "pca.fit(emb_array)\n",
    "with open(\"zattrPCA.pkl\", \"wb\") as file:\n",
    "    pickle.dump(pca, file)\n",
    "\n",
    "pca_array = pca.transform(emb_array)\n",
    "\n",
    "pca_minmax = dict({\"min\": np.amin(pca_array, axis=0), \"max\": np.amax(pca_array, axis=0)})\n",
    "\n",
    "with open(\"zattrPCAMinMax.pkl\", \"wb\") as file:\n",
    "    pickle.dump(pca_minmax, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "985152a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9977929721355753\n",
      "0.9799507086672928\n",
      "0.8972457271133294\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for attr_id in [1, 2, 3]:\n",
    "    emb_array = []\n",
    "\n",
    "    emb_file_list = glob.glob(\"./examples/z_embeds/VggFace2-crop/*.pkl\")\n",
    "\n",
    "    for file_path in emb_file_list:\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                emb_array.append(pickle.load(file)[attr_id].reshape([-1]))\n",
    "    emb_array = np.array(emb_array)\n",
    "\n",
    "    pca = PCA(n_components=1024, svd_solver=\"auto\")\n",
    "\n",
    "    pca.fit(emb_array)\n",
    "    print(np.sum(pca.explained_variance_ratio_))\n",
    "    with open(f\"zattr{attr_id}PCA.pkl\", \"wb\") as file:\n",
    "        pickle.dump(pca, file)\n",
    "\n",
    "    max_emb = []\n",
    "    min_emb = []\n",
    "\n",
    "    for i in range(emb_array.shape[0]):\n",
    "        tmp = emb_array[i:i+1]\n",
    "        pca_array = pca.transform(tmp)\n",
    "        if len(max_emb) == 0:\n",
    "            max_emb = pca_array\n",
    "        else:\n",
    "            max_emb = np.maximum(max_emb, pca_array)\n",
    "        if len(min_emb) == 0:\n",
    "            min_emb = pca_array\n",
    "        else:\n",
    "            min_emb = np.minimum(min_emb, pca_array)\n",
    "\n",
    "    pca_minmax = dict({\"min\": min_emb[0], \"max\": max_emb[0]})\n",
    "\n",
    "    with open(f\"zattr{attr_id}PCAMinMax.pkl\", \"wb\") as file:\n",
    "        pickle.dump(pca_minmax, file)\n",
    "    \n",
    "    del pca, emb_array, pca_array, max_emb, min_emb, pca_minmax, tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4775415",
   "metadata": {},
   "source": [
    "# PCA for zattr[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b017fdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571562172060723\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "attr_id = 4\n",
    "\n",
    "\n",
    "emb_array = []\n",
    "\n",
    "emb_file_list = glob.glob(\"./examples/z_embeds/VggFace2-crop/*.pkl\")\n",
    "\n",
    "for file_path in emb_file_list:\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        emb_array.append(pickle.load(file)[attr_id].reshape([-1]))\n",
    "emb_array = np.array(emb_array)\n",
    "\n",
    "pca = PCA(n_components=1024, svd_solver=\"auto\")\n",
    "\n",
    "pca.fit(emb_array)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "with open(f\"zattr{attr_id}PCA.pkl\", \"wb\") as file:\n",
    "    pickle.dump(pca, file)\n",
    "\n",
    "pca_array = pca.transform(emb_array)\n",
    "\n",
    "pca_minmax = dict({\"min\": np.amin(pca_array, axis=0), \"max\": np.amax(pca_array, axis=0)})\n",
    "\n",
    "with open(f\"zattr{attr_id}PCAMinMax.pkl\", \"wb\") as file:\n",
    "    pickle.dump(pca_minmax, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6323935a",
   "metadata": {},
   "source": [
    "# PCA for zattr[5~6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b4ec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8293578069785364\n",
      "0.8965594179673989\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for attr_id in [5, 6]:\n",
    "    emb_array = []\n",
    "\n",
    "    emb_file_list = glob.glob(\"./examples/z_embeds/VggFace2-crop/*.pkl\")\n",
    "\n",
    "    for file_path in emb_file_list:\n",
    "        if np.random.random() < 0.8:\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                emb_array.append(pickle.load(file)[attr_id].reshape([-1]))\n",
    "    emb_array = np.array(emb_array)\n",
    "\n",
    "    pca = PCA(n_components=1024, svd_solver=\"auto\")\n",
    "\n",
    "    pca.fit(emb_array)\n",
    "    print(np.sum(pca.explained_variance_ratio_))\n",
    "    with open(f\"zattr{attr_id}PCA.pkl\", \"wb\") as file:\n",
    "        pickle.dump(pca, file)\n",
    "\n",
    "    max_emb = []\n",
    "    min_emb = []\n",
    "\n",
    "    for i in range(emb_array.shape[0]):\n",
    "        tmp = emb_array[i:i+1]\n",
    "        pca_array = pca.transform(tmp)\n",
    "        if len(max_emb) == 0:\n",
    "            max_emb = pca_array\n",
    "        else:\n",
    "            max_emb = np.maximum(max_emb, pca_array)\n",
    "        if len(min_emb) == 0:\n",
    "            min_emb = pca_array\n",
    "        else:\n",
    "            min_emb = np.minimum(min_emb, pca_array)\n",
    "\n",
    "    pca_minmax = dict({\"min\": min_emb[0], \"max\": max_emb[0]})\n",
    "\n",
    "    with open(f\"zattr{attr_id}PCAMinMax.pkl\", \"wb\") as file:\n",
    "        pickle.dump(pca_minmax, file)\n",
    "    \n",
    "    del pca, emb_array, pca_array, max_emb, min_emb, pca_minmax, tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117f318e",
   "metadata": {},
   "source": [
    "# PCA for zattr[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf52209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8869378560189847\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "attr_id = 7\n",
    "\n",
    "\n",
    "emb_array = []\n",
    "\n",
    "emb_file_list = glob.glob(\"./examples/z_embeds/VggFace2-crop/*.pkl\")\n",
    "\n",
    "for file_path in emb_file_list[::2]:\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        emb_array.append(pickle.load(file)[attr_id].reshape([-1]))\n",
    "emb_array = np.array(emb_array)\n",
    "\n",
    "pca = PCA(n_components=512, svd_solver=\"auto\")\n",
    "\n",
    "pca.fit(emb_array)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "with open(f\"zattr{attr_id}PCA.pkl\", \"wb\") as file:\n",
    "    pickle.dump(pca, file)\n",
    "\n",
    "\n",
    "\n",
    "max_emb = []\n",
    "min_emb = []\n",
    "\n",
    "for i in range(emb_array.shape[0]):\n",
    "    tmp = emb_array[i:i+1]\n",
    "    pca_array = pca.transform(tmp)\n",
    "    if len(max_emb) == 0:\n",
    "        max_emb = pca_array\n",
    "    else:\n",
    "        max_emb = np.maximum(max_emb, pca_array)\n",
    "    if len(min_emb) == 0:\n",
    "        min_emb = pca_array\n",
    "    else:\n",
    "        min_emb = np.minimum(min_emb, pca_array)\n",
    "\n",
    "pca_minmax = dict({\"min\": min_emb[0], \"max\": max_emb[0]})\n",
    "\n",
    "with open(f\"zattr{attr_id}PCAMinMax.pkl\", \"wb\") as file:\n",
    "    pickle.dump(pca_minmax, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d74232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def nop(it, *a, **k):\n",
    "    return it\n",
    "\n",
    "real_tqdm = tqdm.tqdm\n",
    "tqdm.tqdm = nop\n",
    "\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "attr_id = 7\n",
    "\n",
    "with open(f\"zattr{attr_id}PCA.pkl\", \"rb\") as file:\n",
    "    pca = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "emb_file_list = glob.glob(\"./examples/z_embeds/VggFace2-crop/*.pkl\")\n",
    "\n",
    "max_emb = []\n",
    "min_emb = []\n",
    "\n",
    "for file_path in real_tqdm(emb_file_list):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        tmp = pickle.load(file)[attr_id].reshape([1,-1])\n",
    "    \n",
    "    pca_array = pca.transform(tmp)\n",
    "    if len(max_emb) == 0:\n",
    "        max_emb = pca_array\n",
    "    else:\n",
    "        max_emb = np.maximum(max_emb, pca_array)\n",
    "    if len(min_emb) == 0:\n",
    "        min_emb = pca_array\n",
    "    else:\n",
    "        min_emb = np.minimum(min_emb, pca_array)\n",
    "\n",
    "pca_minmax = dict({\"min\": min_emb[0], \"max\": max_emb[0]})\n",
    "\n",
    "with open(f\"zattr{attr_id}PCAMinMax.pkl\", \"wb\") as file:\n",
    "    pickle.dump(pca_minmax, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86fd3d7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PCA' object has no attribute 'components'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponents\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PCA' object has no attribute 'components'"
     ]
    }
   ],
   "source": [
    "pca.components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e50b8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaiseung_ghost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
