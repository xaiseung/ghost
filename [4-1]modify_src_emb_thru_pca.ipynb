{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def nop(it, *a, **k):\n",
    "    return it\n",
    "\n",
    "real_tqdm = tqdm.tqdm\n",
    "tqdm.tqdm = nop\n",
    "\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "import cv2\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.inference.image_processing import crop_face, get_final_image, show_images, normalize_and_torch, normalize_and_torch_batch\n",
    "from utils.inference.video_processing import read_video, get_target, get_final_video, add_audio_from_another_video, face_enhancement, crop_frames_and_get_transforms, resize_frames\n",
    "from utils.inference.core import model_inference, transform_target_to_torch\n",
    "from utils.inference.faceshifter_run import faceshifter_batch\n",
    "from network.AEI_Net import AEI_Net\n",
    "from coordinate_reg.image_infer import Handler\n",
    "from insightface_func.face_detect_crop_multi import Face_detect_crop\n",
    "from arcface_model.iresnet import iresnet100\n",
    "from models.pix2pix_model import Pix2PixModel\n",
    "from models.config_sr import TestOptions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-factor",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Face_detect_crop(name='antelope', root='./insightface_func/models')\n",
    "app.prepare(ctx_id= 0, det_thresh=0.6, det_size=(640,640))\n",
    "\n",
    "# main model for generation\n",
    "G = AEI_Net(backbone='unet', num_blocks=2, c_id=512)\n",
    "G.eval()\n",
    "G.load_state_dict(torch.load('weights/G_unet_2blocks.pth', map_location=torch.device('cpu')))\n",
    "G = G.cuda()\n",
    "G = G.half()\n",
    "\n",
    "# arcface model to get face embedding\n",
    "netArc = iresnet100(fp16=False)\n",
    "netArc.load_state_dict(torch.load('arcface_model/backbone.pth'))\n",
    "netArc=netArc.cuda()\n",
    "netArc.eval()\n",
    "\n",
    "# model to get face landmarks\n",
    "handler = Handler('./coordinate_reg/model/2d106det', 0, ctx_id=0, det_size=640)\n",
    "\n",
    "# model to make superres of face, set use_sr=True if you want to use super resolution or use_sr=False if you don't\n",
    "use_sr = True\n",
    "if use_sr:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    opt = TestOptions()\n",
    "    #opt.which_epoch ='10_7'\n",
    "    model = Pix2PixModel(opt)\n",
    "    model.netG.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-hanging",
   "metadata": {},
   "source": [
    "### Set here path to source image and video for faceswap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d613e79",
   "metadata": {},
   "source": [
    "# 특징 빼기 + 인젝션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d599af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.inference.faceshifter_run import faceshifter_batch\n",
    "from utils.inference.image_processing import crop_face, normalize_and_torch, normalize_and_torch_batch\n",
    "from utils.inference.video_processing import read_video, crop_frames_and_get_transforms, resize_frames\n",
    "from utils.inference.core import transform_target_to_torch\n",
    "\n",
    "image_to_image = True\n",
    "\n",
    "set_target = False\n",
    "half=True\n",
    "similarity_th=0.15\n",
    "\"\"\"\n",
    "choose not really long videos, coz it can take a lot of time othervise \n",
    "choose source image as a photo -- preferable a selfie of a person\n",
    "\"\"\"\n",
    "if image_to_image:\n",
    "    path_to_target = 'examples/images/jaeseung.jpg'\n",
    "else:\n",
    "    #path_to_video = 'examples/videos/random_gif.gif'\n",
    "    path_to_video = \"examples/videos/01__hugging_happy.mp4\"\n",
    "#source_full = cv2.imread('examples/images/elon_musk.jpg')\n",
    "source_full = cv2.imread('examples/images/Bob-Ross.webp')\n",
    "OUT_VIDEO_NAME = \"examples/results/result_tmp.mp4\"\n",
    "crop_size = 224 # don't change this\n",
    "BS = 60\n",
    "# check, if we can detect face on the source image\n",
    "\n",
    "try:    \n",
    "    source = crop_face(source_full, app, crop_size)[0]\n",
    "    print(source.shape)\n",
    "    source = [source[:, :, ::-1]]\n",
    "    print(\"Everything is ok!\")\n",
    "except TypeError:\n",
    "    print(\"Bad source images\")\n",
    "    \n",
    "# read video and find target image in the video that contains at least 1 face\n",
    "\n",
    "if image_to_image:\n",
    "    target_full = cv2.imread(path_to_target)\n",
    "    full_frames = [target_full]\n",
    "else:\n",
    "    full_frames, fps = read_video(path_to_video)\n",
    "target = get_target(full_frames, app, crop_size)\n",
    "\n",
    "target_norm = normalize_and_torch_batch(np.array(target))\n",
    "target_embeds = netArc(F.interpolate(target_norm, scale_factor=0.5, mode='bilinear', align_corners=True))\n",
    "\n",
    "# Get the cropped faces from original frames and transformations to get those crops\n",
    "crop_frames_list, tfm_array_list = crop_frames_and_get_transforms(full_frames, target_embeds, app, netArc, crop_size, set_target, similarity_th=similarity_th)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec1e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_target = False\n",
    "half=True\n",
    "similarity_th=0.15\n",
    "\n",
    "\n",
    "target_norm = normalize_and_torch_batch(np.array(target))\n",
    "target_embeds = netArc(F.interpolate(target_norm, scale_factor=0.5, mode='bilinear', align_corners=True))\n",
    "\n",
    "# Get the cropped faces from original frames and transformations to get those crops\n",
    "crop_frames_list, tfm_array_list = crop_frames_and_get_transforms(full_frames, target_embeds, app, netArc, crop_size, set_target, similarity_th=similarity_th)\n",
    "\n",
    "# Normalize source images and transform to torch and get Arcface embeddings\n",
    "source_embeds = []\n",
    "for source_curr in source:\n",
    "    source_curr = normalize_and_torch(source_curr)\n",
    "    source_embeds.append(netArc(F.interpolate(source_curr, scale_factor=0.5, mode='bilinear', align_corners=True)))\n",
    "\n",
    "print(source_embeds[0].shape)\n",
    "\n",
    "print(crop_frames_list[0][0].shape, crop_frames_list[0][0].dtype)\n",
    "\n",
    "plt.imshow(crop_frames_list[0][0])\n",
    "plt.show()\n",
    "\n",
    "final_frames_list = []\n",
    "for idx, (crop_frames, tfm_array, source_embed) in enumerate(zip(crop_frames_list, tfm_array_list, source_embeds)):\n",
    "    # Resize croped frames and get vector which shows on which frames there were faces\n",
    "    resized_frs, present = resize_frames(crop_frames)\n",
    "    resized_frs = np.array(resized_frs)\n",
    "\n",
    "    # transform embeds of Xs and target frames to use by model\n",
    "    target_batch_rs = transform_target_to_torch(resized_frs, half=half)\n",
    "    print(target_batch_rs.shape)\n",
    "    #assert False\n",
    "    if half:\n",
    "        source_embed = source_embed.half()\n",
    "\n",
    "    # run model\n",
    "    size = target_batch_rs.shape[0]\n",
    "    model_output = []\n",
    "\n",
    "    for i in tqdm(range(0, size, BS)):\n",
    "        Y_st = faceshifter_batch(source_embed, target_batch_rs[i:i+BS], G)\n",
    "        model_output.append(Y_st)\n",
    "    torch.cuda.empty_cache()\n",
    "    model_output = np.concatenate(model_output)\n",
    "\n",
    "    # create list of final frames with transformed faces\n",
    "    final_frames = []\n",
    "    idx_fs = 0\n",
    "\n",
    "    for pres in tqdm(present):\n",
    "        if pres == 1:\n",
    "            final_frames.append(model_output[idx_fs])\n",
    "            idx_fs += 1\n",
    "        else:\n",
    "            final_frames.append([])\n",
    "    final_frames_list.append(final_frames)\n",
    "    assert False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_frames[0].shape\n",
    "\n",
    "plt.imshow(Y_st[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f12e1f",
   "metadata": {},
   "source": [
    "# pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66534486",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"netArcPCA2.pkl\", \"rb\") as file:\n",
    "    pca = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ffd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.inference.core import transform_target_to_torch\n",
    "\n",
    "#frame = cv2.imread(\"../vggFace2_Train/n000002/0002_01.jpg\")\n",
    "\n",
    "#crop_frames_list = [[cv2.resize(frame, (crop_size, crop_size))]]\n",
    "\n",
    "print(crop_frames_list[0][0].shape)\n",
    "plt.imshow(crop_frames_list[0][0][:,:,::-1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "resized_frs, present = resize_frames(crop_frames_list[0])\n",
    "resized_frs = np.array(resized_frs)\n",
    "\n",
    "target_batch_rs = transform_target_to_torch(resized_frs, half=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "source_full = cv2.imread(pic)\n",
    "#print(source_full.shape)\n",
    "#source = crop_face(source_full, app, crop_size)[0]\n",
    "source_curr = cv2.resize(source_full[:, :, ::-1], (crop_size, crop_size))\n",
    "source_curr = normalize_and_torch(source_curr)\n",
    "#print(source_curr.shape)\n",
    "source_embed = netArc(F.interpolate(source_curr, scale_factor=0.5, mode='bilinear', align_corners=True))\n",
    "source_embed = source_embed.half()\n",
    "\n",
    "\"\"\"\n",
    "start_id = 7\n",
    "tmp_embed = [np.load(\"./embeds/{}.npy\".format(start_id))]\n",
    "pca_array = pca.transform(tmp_embed)\n",
    "\n",
    "modified_embed = pca.inverse_transform(pca_array)\n",
    "\n",
    "source_embed = torch.from_numpy(modified_embed).half().to(\"cuda\")\n",
    "#source_embed[0][0] = 10.0\n",
    "\n",
    "Y_st = faceshifter_batch(source_embed, target_batch_rs, G)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "plt.imshow(Y_st[0][:, :, ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from insightface.utils import face_align\n",
    "import ipywidgets as widgets\n",
    "\n",
    "set_target = False\n",
    "half=True\n",
    "similarity_th=0.15\n",
    "set_target = False\n",
    "half=True\n",
    "similarity_th=0.15\n",
    "\n",
    "crop_frames_list = None\n",
    "target_batch_rs = None\n",
    "start_id = 0\n",
    "tmp_embed = [np.load(\"./embeds/{}.npy\".format(start_id))]\n",
    "pca_array = pca.transform(tmp_embed)\n",
    "with open(\"netArcPCA2MinMax.pkl\", \"rb\") as file:\n",
    "    pca_minmax = pickle.load(file)\n",
    "pca_min = pca_minmax[\"min\"]\n",
    "pca_max = pca_minmax[\"max\"]\n",
    "\n",
    "def set_target(path_to_target='examples/images/jaeseung.jpg'):\n",
    "    global crop_frames_list, target_batch_rs\n",
    "    if image_to_image:\n",
    "        target_full = cv2.imread(path_to_target)\n",
    "        full_frames = [target_full]\n",
    "    else:\n",
    "        full_frames, fps = read_video(path_to_video)\n",
    "    target = get_target(full_frames, app, crop_size)\n",
    "\n",
    "    target_norm = normalize_and_torch_batch(np.array(target))\n",
    "    target_embeds = netArc(F.interpolate(target_norm, scale_factor=0.5, mode='bilinear', align_corners=True))\n",
    "\n",
    "    # Get the cropped faces from original frames and transformations to get those crops\n",
    "    crop_frames_list, tfm_array_list = crop_frames_and_get_transforms(full_frames,\n",
    "                                                                    target_embeds,\n",
    "                                                                    app,\n",
    "                                                                    netArc,\n",
    "                                                                    crop_size,\n",
    "                                                                    set_target,\n",
    "                                                                    similarity_th=similarity_th\n",
    "                                                                    )\n",
    "    resized_frs, present = resize_frames(crop_frames_list[0])\n",
    "    resized_frs = np.array(resized_frs)\n",
    "\n",
    "    target_batch_rs = transform_target_to_torch(resized_frs, half=True)\n",
    "\n",
    "\n",
    "set_target()\n",
    "\n",
    "\n",
    "def inject_drawing():\n",
    "    global crop_frames_list, pca_array\n",
    "    plt.figure(num=1, clear=True, figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(crop_frames_list[0][0][:,:,::-1])\n",
    "    plt.title(\"Target Face\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    modified_embed = pca.inverse_transform(pca_array)\n",
    "\n",
    "    source_embed = torch.from_numpy(modified_embed).half().to(\"cuda\")\n",
    "    #source_embed[0][0] = 10.0\n",
    "\n",
    "    Y_st = faceshifter_batch(source_embed, target_batch_rs, G)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    plt.imshow(Y_st[0][:, :, ::-1])\n",
    "    plt.title(\"After Swap\")\n",
    "    plt.show()\n",
    "\n",
    "pca_sliders = []\n",
    "\n",
    "iact_plot = widgets.interactive(\n",
    "    inject_drawing\n",
    ")\n",
    "\n",
    "for i in range(40):\n",
    "    pca_slider = widgets.FloatSlider(\n",
    "        value=pca_array[0, i], \n",
    "        min = pca_min[i],\n",
    "        max = pca_max[i],\n",
    "        description=f\"PCA #{i}\",\n",
    "        continuous_update=False,\n",
    "        layout=widgets.Layout(width=\"300px\"),\n",
    "    )\n",
    "    pca_slider.idx = i\n",
    "    pca_slider.observe(lambda change: exec(\"pca_array[0, change.owner.idx]=change.new;iact_plot.update()\"), names=\"value\")\n",
    "    #pca_slider.observe(lambda change: print(change.owner.idx), names=\"value\")\n",
    "    pca_sliders.append(pca_slider)\n",
    "    \n",
    "\n",
    "slider_multibox = widgets.HBox(\n",
    "    children=[\n",
    "    widgets.VBox(\n",
    "    children = pca_sliders[:20]),\n",
    "    widgets.VBox(\n",
    "    children = pca_sliders[20:]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "display(iact_plot, slider_multibox)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fc7da",
   "metadata": {},
   "source": [
    "# PCA 값에 따른 변화 그림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73efa0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_target = False\n",
    "half=True\n",
    "similarity_th=0.15\n",
    "set_target = False\n",
    "half=True\n",
    "similarity_th=0.15\n",
    "\n",
    "crop_frames_list = None\n",
    "target_batch_rs = None\n",
    "with open(\"netArcPCA2.pkl\", \"rb\") as file:\n",
    "    pca = pickle.load(file)\n",
    "\n",
    "with open(\"netArcPCA2MinMax.pkl\", \"rb\") as file:\n",
    "    pca_minmax = pickle.load(file)\n",
    "pca_min = pca_minmax[\"min\"]\n",
    "pca_max = pca_minmax[\"max\"]\n",
    "\n",
    "start_id = 0\n",
    "#tmp_embed = [np.load(\"./embeds/{}.npy\".format(start_id))]\n",
    "#pca_array = pca.transform(tmp_embed)\n",
    "\n",
    "tmp_embed = [((0.3+0.7*np.random.random(pca_min.shape[0]))*(pca_max-pca_min)+pca_min)]\n",
    "#tmp_embed = [(pca_min+pca_max)/2]\n",
    "pca_array = pca.transform(tmp_embed)\n",
    "\n",
    "\n",
    "def set_target(path_to_target='examples/images/jaeseung.jpg'):\n",
    "    global crop_frames_list, target_batch_rs\n",
    "    if image_to_image:\n",
    "        target_full = cv2.imread(path_to_target)\n",
    "        full_frames = [target_full]\n",
    "    else:\n",
    "        full_frames, fps = read_video(path_to_video)\n",
    "    target = get_target(full_frames, app, crop_size)\n",
    "\n",
    "    target_norm = normalize_and_torch_batch(np.array(target))\n",
    "    target_embeds = netArc(F.interpolate(target_norm, scale_factor=0.5, mode='bilinear', align_corners=True))\n",
    "\n",
    "    # Get the cropped faces from original frames and transformations to get those crops\n",
    "    crop_frames_list, tfm_array_list = crop_frames_and_get_transforms(full_frames,\n",
    "                                                                    target_embeds,\n",
    "                                                                    app,\n",
    "                                                                    netArc,\n",
    "                                                                    crop_size,\n",
    "                                                                    set_target,\n",
    "                                                                    similarity_th=similarity_th\n",
    "                                                                    )\n",
    "    resized_frs, present = resize_frames(crop_frames_list[0])\n",
    "    resized_frs = np.array(resized_frs)\n",
    "\n",
    "    target_batch_rs = transform_target_to_torch(resized_frs, half=True)\n",
    "\n",
    "\n",
    "set_target(path_to_target=\"examples/images/jaeseung.jpg\")\n",
    "\n",
    "num_plotted_pca = 40\n",
    "n_ticks = 5\n",
    "\n",
    "plt.figure(num=1, clear=True, figsize=(n_ticks*2, num_plotted_pca*2))\n",
    "plt.subplot(num_plotted_pca+1,n_ticks, 1)\n",
    "plt.imshow(crop_frames_list[0][0][:,:,::-1])\n",
    "plt.title(\"Target Face\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(num_plotted_pca+1,n_ticks,2)\n",
    "\n",
    "modified_embed = pca.inverse_transform(pca_array)\n",
    "\n",
    "source_embed = torch.from_numpy(modified_embed).half().to(\"cuda\")\n",
    "#source_embed[0][0] = 10.0\n",
    "\n",
    "Y_st = faceshifter_batch(source_embed, target_batch_rs, G)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "plt.imshow(Y_st[0][:, :, ::-1])\n",
    "plt.title(\"Swapped (Random)\")\n",
    "#plt.axis(\"off\")\n",
    "for side in [\"top\", \"right\", \"bottom\", \"left\"]: plt.gca().spines[side].set_visible(False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "\n",
    "\n",
    "for pci_i in range(num_plotted_pca):\n",
    "    interval = (pca_max[pci_i] - pca_min[pci_i]) / (n_ticks-1)\n",
    "    for c in range(n_ticks):\n",
    "        plt.subplot(num_plotted_pca+1,n_ticks, (pci_i+1)*n_ticks+c+1)\n",
    "        new_pca_array = pca_array.copy()\n",
    "        new_pca_array[0, pci_i] = pca_min[pci_i] + (interval * c)\n",
    "        modified_embed = pca.inverse_transform(new_pca_array)\n",
    "\n",
    "        source_embed = torch.from_numpy(modified_embed).half().to(\"cuda\")\n",
    "        #source_embed[0][0] = 10.0\n",
    "\n",
    "        Y_st = faceshifter_batch(source_embed, target_batch_rs, G)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        plt.imshow(Y_st[0][:, :, ::-1])\n",
    "        #plt.title(\"Swapped Face \")\n",
    "        #plt.axis(\"off\")\n",
    "        for side in [\"top\", \"right\", \"bottom\", \"left\"]: plt.gca().spines[side].set_visible(False)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.xlabel(f\"{new_pca_array[0, pci_i]:.2f}\")\n",
    "\n",
    "        if c == 0:\n",
    "            plt.ylabel(\"PCA #{}\".format(pci_i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison\n",
    "\n",
    "set_target = False\n",
    "half=True\n",
    "similarity_th=0.15\n",
    "crop_size = 224 # don't change this\n",
    "BS = 60\n",
    "\n",
    "\n",
    "crop_frames_list = None\n",
    "target_batch_rs = None\n",
    "with open(\"netArcPCA2.pkl\", \"rb\") as file:\n",
    "    pca = pickle.load(file)\n",
    "\n",
    "with open(\"netArcPCA2MinMax.pkl\", \"rb\") as file:\n",
    "    pca_minmax = pickle.load(file)\n",
    "pca_min = pca_minmax[\"min\"]\n",
    "pca_max = pca_minmax[\"max\"]\n",
    "\n",
    "start_id = 0\n",
    "#tmp_embed = [np.load(\"./embeds/{}.npy\".format(start_id))]\n",
    "#pca_array = pca.transform(tmp_embed)\n",
    "\n",
    "tmp_embed = [((0.4+0.6*np.random.random(pca_min.shape[0]))*(pca_max-pca_min)+pca_min)]\n",
    "#tmp_embed = [(pca_min+pca_max)/2]\n",
    "pca_array = pca.transform(tmp_embed)\n",
    "\n",
    "\n",
    "def set_target(path_to_target='examples/images/jaeseung.jpg'):\n",
    "    global crop_frames_list, target_batch_rs\n",
    "    target_full = cv2.imread(path_to_target)\n",
    "    full_frames = [target_full]\n",
    "    target = get_target(full_frames, app, crop_size)\n",
    "\n",
    "    target_norm = normalize_and_torch_batch(np.array(target))\n",
    "    target_embeds = netArc(F.interpolate(target_norm, scale_factor=0.5, mode='bilinear', align_corners=True))\n",
    "\n",
    "    # Get the cropped faces from original frames and transformations to get those crops\n",
    "    crop_frames_list, tfm_array_list = crop_frames_and_get_transforms(full_frames,\n",
    "                                                                    target_embeds,\n",
    "                                                                    app,\n",
    "                                                                    netArc,\n",
    "                                                                    crop_size,\n",
    "                                                                    set_target,\n",
    "                                                                    similarity_th=similarity_th\n",
    "                                                                    )\n",
    "    resized_frs, present = resize_frames(crop_frames_list[0])\n",
    "    resized_frs = np.array(resized_frs)\n",
    "\n",
    "    target_batch_rs = transform_target_to_torch(resized_frs, half=True)\n",
    "\n",
    "\n",
    "set_target(path_to_target=\"examples/images/jaeseung.jpg\")\n",
    "\n",
    "num_plotted_pca = 20\n",
    "n_ticks = 7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(num=1, clear=True, figsize=(n_ticks*2, (num_plotted_pca*3+1)*2))\n",
    "\n",
    "def draw_top(subplot_tuple, img):\n",
    "    plt.subplot(*subplot_tuple)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Target Face\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "def draw_swap(subplot_tuple, pca_embed, is_top=False):\n",
    "    global target_batch_rs, G\n",
    "    plt.subplot(*subplot_tuple)\n",
    "\n",
    "    modified_embed = pca.inverse_transform(pca_embed)\n",
    "    source_embed = torch.from_numpy(modified_embed).half().to(\"cuda\")\n",
    "\n",
    "    Y_st = faceshifter_batch(source_embed, target_batch_rs, G)\n",
    "    torch.cuda.empty_cache()\n",
    "    plt.imshow(Y_st[0][:, :, ::-1])\n",
    "    \n",
    "    for side in [\"top\", \"right\", \"bottom\", \"left\"]: plt.gca().spines[side].set_visible(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if is_top:\n",
    "        plt.title(\"Swapped (Random)\")\n",
    "\n",
    "\n",
    "set_target(path_to_target=\"examples/images/jaeseung.jpg\")\n",
    "draw_top((num_plotted_pca*3+1, n_ticks, 1), crop_frames_list[0][0][:,:,::-1])\n",
    "draw_swap((num_plotted_pca*3+1, n_ticks, 2), pca_array, is_top=True)\n",
    "\n",
    "set_target(path_to_target=\"examples/images/elon_musk.jpg\")\n",
    "draw_top((num_plotted_pca*3+1, n_ticks, 3), crop_frames_list[0][0][:,:,::-1])\n",
    "draw_swap((num_plotted_pca*3+1, n_ticks, 4), pca_array, is_top=True)\n",
    "\n",
    "set_target(path_to_target=\"examples/images/tgt2.png\")\n",
    "draw_top((num_plotted_pca*3+1, n_ticks, 5), crop_frames_list[0][0][:,:,::-1])\n",
    "draw_swap((num_plotted_pca*3+1, n_ticks, 6), pca_array, is_top=True)\n",
    "\n",
    "for pci_i in range(num_plotted_pca):\n",
    "    interval = (pca_max[pci_i] - pca_min[pci_i]) / (n_ticks-1)\n",
    "    for c in range(n_ticks):\n",
    "        new_pca_array = pca_array.copy()\n",
    "        new_pca_array[0, pci_i] = pca_min[pci_i] + (interval * c)\n",
    "        \n",
    "        set_target(path_to_target=\"examples/images/jaeseung.jpg\")\n",
    "        draw_swap((num_plotted_pca*3+1, n_ticks, (pci_i*3+1)*n_ticks+c+1), new_pca_array)\n",
    "        plt.xlabel(f\"{new_pca_array[0, pci_i]:.2f}\")\n",
    "        if c == 0:\n",
    "            plt.ylabel(\"PCA #{}\".format(pci_i))\n",
    "        set_target(path_to_target=\"examples/images/elon_musk.jpg\")\n",
    "        draw_swap((num_plotted_pca*3+1, n_ticks, (pci_i*3+2)*n_ticks+c+1), new_pca_array)\n",
    "        plt.xlabel(f\"{new_pca_array[0, pci_i]:.2f}\")\n",
    "        if c == 0:\n",
    "            plt.ylabel(\"PCA #{}\".format(pci_i))\n",
    "        set_target(path_to_target=\"examples/images/tgt2.png\")\n",
    "        draw_swap((num_plotted_pca*3+1, n_ticks, (pci_i*3+3)*n_ticks+c+1), new_pca_array)\n",
    "        plt.xlabel(f\"{new_pca_array[0, pci_i]:.2f}\")\n",
    "        if c == 0:\n",
    "            plt.ylabel(\"PCA #{}\".format(pci_i))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaiseung_ghost_cuda114",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
