{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/kornia/augmentation/augmentation.py:1830: DeprecationWarning: GaussianBlur is no longer maintained and will be removed from the future versions. Please use RandomGaussianBlur instead.\n",
      "  warnings.warn(\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:143: DeprecationWarning: In accordance with NEP 32, the function mirr was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  mirr = onp.mirr\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:160: DeprecationWarning: In accordance with NEP 32, the function npv was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  npv = onp.npv\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:164: DeprecationWarning: In accordance with NEP 32, the function pmt was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  pmt = onp.pmt\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:173: DeprecationWarning: In accordance with NEP 32, the function ppmt was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  ppmt = onp.ppmt\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:176: DeprecationWarning: In accordance with NEP 32, the function pv was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  pv = onp.pv\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:177: DeprecationWarning: In accordance with NEP 32, the function rate was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  rate = onp.rate\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy_dispatch_protocol.py:48: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  cur_np_ver = LooseVersion(_np.__version__)\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy_dispatch_protocol.py:49: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  np_1_17_ver = LooseVersion('1.17')\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy_dispatch_protocol.py:68: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  cur_np_ver = LooseVersion(_np.__version__)\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy_dispatch_protocol.py:69: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  np_1_15_ver = LooseVersion('1.15')\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "def nop(it, *a, **k):\n",
    "    return it\n",
    "\n",
    "real_tqdm = tqdm.tqdm\n",
    "tqdm.tqdm = nop\n",
    "\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "import cv2\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.inference.image_processing import crop_face, get_final_image, show_images, normalize_and_torch, normalize_and_torch_batch\n",
    "from utils.inference.video_processing import read_video, get_target, get_final_video, add_audio_from_another_video, face_enhancement, crop_frames_and_get_transforms, resize_frames\n",
    "from utils.inference.core import model_inference, transform_target_to_torch\n",
    "from utils.inference.faceshifter_run import faceshifter_batch, faceshifter_batch_zattrs\n",
    "from network.AEI_Net import AEI_Net\n",
    "from coordinate_reg.image_infer import Handler\n",
    "from insightface_func.face_detect_crop_multi import Face_detect_crop\n",
    "from arcface_model.iresnet import iresnet100\n",
    "from models.pix2pix_model import Pix2PixModel\n",
    "from models.config_sr import TestOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoublePCA:\n",
    "    def __init__(self, layer_num=8, root_path=\"./pca_pkl\"):\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "        self.pca1_list = []\n",
    "        for z_i in range(self.layer_num):\n",
    "            with open(f\"{root_path}/Altered_zattr{z_i}PCA.pkl\", \"rb\") as file:\n",
    "                self.pca1_list.append(pickle.load(file))\n",
    "        with open(f\"{root_path}/Altered_zattr_doublePCA.pkl\", \"rb\") as file:\n",
    "            self.pca2 = pickle.load(file)\n",
    "        with open(f\"{root_path}/Altered_zattr_doublePCAMinMax.pkl\", \"rb\") as file:\n",
    "            minmax = pickle.load(file)\n",
    "        self.pca2_min = minmax[\"min\"]\n",
    "        self.pca2_max = minmax[\"max\"]\n",
    "    def transform(self, z_embeds):\n",
    "        p1emb_array = [self.pca1_list[z_i].transform(z_embeds[z_i]) for z_i in range(self.layer_num)]\n",
    "        p1emb_array = np.concatenate(p1emb_array, axis=1)\n",
    "        return self.pca2.transform(p1emb_array)\n",
    "    def inverse_transform(self, p2emb_array):\n",
    "        p1emb_array = self.pca2.inverse_transform(p2emb_array).reshape([-1, self.layer_num, 128])\n",
    "        z_embeds = [self.pca1_list[z_i].inverse_transform(p1emb_array[:,z_i]) for z_i in range(self.layer_num)]\n",
    "        return z_embeds\n",
    "    def calcul_z_embed_diff(self, target, original):\n",
    "        assert target.shape[0] == original.shape[0]\n",
    "        target = self.inverse_transform(target)\n",
    "        original = self.inverse_transform(original)\n",
    "        return [target[i]-original[i] for i in range(self.layer_num)]\n",
    "\n",
    "\n",
    "\n",
    "class FaceSwapper():\n",
    "    def __init__(self):\n",
    "        self.crop_size = 224 \n",
    "\n",
    "        self.app = Face_detect_crop(name='antelope', root='./insightface_func/models')\n",
    "        self.app.prepare(ctx_id= 0, det_thresh=0.6, det_size=(640,640))\n",
    "\n",
    "        # main model for generation\n",
    "        self.G = AEI_Net(backbone='unet', num_blocks=2, c_id=512)\n",
    "        self.G.eval()\n",
    "        self.G.load_state_dict(torch.load('weights/G_unet_2blocks.pth', map_location=torch.device('cpu')))\n",
    "        self.G = G.cuda()\n",
    "        self.G = G.half()\n",
    "\n",
    "        # arcface model to get face embedding\n",
    "        self.netArc = iresnet100(fp16=False)\n",
    "        self.netArc.load_state_dict(torch.load('arcface_model/backbone.pth'))\n",
    "        self.netArc=netArc.cuda()\n",
    "        self.netArc.eval()\n",
    "\n",
    "        # model to get face landmarks\n",
    "        self.handler = Handler('./coordinate_reg/model/2d106det', 0, ctx_id=0, det_size=640)\n",
    "\n",
    "        # model to make superres of face, set use_sr=True if you want to use super resolution or use_sr=False if you don't\n",
    "        \n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        opt = TestOptions()\n",
    "        #opt.which_epoch ='10_7'\n",
    "        self.model = Pix2PixModel(opt)\n",
    "        self.model.netG.train()\n",
    "        \n",
    "        # 중간 결과 임시저장\n",
    "        self.source = None\n",
    "        self.full_frames = None\n",
    "        self.orig_target_embed = None\n",
    "        self.final_z_outputs = None\n",
    "        self.crop_frames_list = None\n",
    "        self.tfm_array_list = None\n",
    "        \n",
    "    \n",
    "    def swap(self, source: Union[np.ndarray, str], target: Union[np.ndarray, str], is_tgt_video=False, BS = 60):\n",
    "        \"\"\" # TODO\n",
    "\n",
    "        source와 target은 imread의 출력, 즉 bgr ndarray 입력으로 간주한다.\n",
    "        단, target이 영상인 경우에는 [t, H, W, C]꼴의 ndarray 리스트 입력으로 생각한다\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if isinstance(source, str):\n",
    "            source = cv2.imread(source)\n",
    "        self.source = source\n",
    "        source_curr = normalize_and_torch(self.source)\n",
    "\n",
    "        if isinstance(target, str):\n",
    "            if is_tgt_video:\n",
    "                full_frames, fps = read_video(tgt)\n",
    "            else:\n",
    "                target_full = cv2.imread(target)\n",
    "                full_frames = [target_full]\n",
    "        else:\n",
    "            \n",
    "            full_frames = target if is_tgt_video else [target]\n",
    "            else:\n",
    "                full_frames = target\n",
    "        self.full_frames = full_frames\n",
    "\n",
    "        cropped_target = get_target(full_frames, self.app, self.crop_size)\n",
    "        target_norm = normalize_and_torch_batch(np.array(cropped_target))\n",
    "        target_embeds = netArc(F.interpolate(target_norm, scale_factor=0.5, mode='bilinear', align_corners=True))\n",
    "        crop_frames_list, tfm_array_list = crop_frames_and_get_transforms(full_frames,\n",
    "                                                                    target_embeds,\n",
    "                                                                    self.app,\n",
    "                                                                    self.netArc,\n",
    "                                                                    self.crop_size,\n",
    "                                                                    set_target=False,\n",
    "                                                                    similarity_th=0.15\n",
    "                                                                    )\n",
    "        self.crop_frames_list = crop_frames_list\n",
    "        self.tfm_array_list = tfm_array_list\n",
    "\n",
    "        source_embeds = []\n",
    "        for source_curr in source:\n",
    "            source_curr = normalize_and_torch(source_curr)\n",
    "            source_embeds.append(netArc(F.interpolate(source_curr, scale_factor=0.5, mode='bilinear', align_corners=True)))\n",
    "\n",
    "        final_frames_list = []\n",
    "        self.final_z_outputs = []\n",
    "        for idx, (crop_frames, tfm_array, source_embed) in enumerate(zip(self.crop_frames_list, self.tfm_array_list, source_embeds)):\n",
    "            # Resize croped frames and get vector which shows on which frames there were faces\n",
    "            resized_frs, present = resize_frames(crop_frames)\n",
    "            resized_frs = np.array(resized_frs)\n",
    "\n",
    "            # transform embeds of Xs and target frames to use by model\n",
    "            target_batch_rs = transform_target_to_torch(resized_frs, half=half)\n",
    "            #assert False\n",
    "            source_embed = source_embed.half()\n",
    "\n",
    "            # run model\n",
    "            size = target_batch_rs.shape[0]\n",
    "            model_output = []\n",
    "            z_output = []\n",
    "            for i in range(0, size, BS):\n",
    "                zattrs = self.G.get_attr(target_batch_rs[i:i+BS])\n",
    "                Y_st = faceshifter_batch_zattrs(source_embed, zattrs, BS, self.G)\n",
    "                model_output.append(Y_st)\n",
    "                z_output.append(zattrs)\n",
    "            torch.cuda.empty_cache()\n",
    "            model_output = np.concatenate(model_output)\n",
    "            z_output = torch.concat(z_output,dim=0)\n",
    "\n",
    "            # create list of final frames with transformed faces\n",
    "            final_frames = []\n",
    "            final_zs = []\n",
    "            idx_fs = 0\n",
    "            for pres in present:\n",
    "                if pres == 1:\n",
    "                    final_frames.append(model_output[idx_fs])\n",
    "                    final_zs.append(z_output[idx_fs])\n",
    "                    idx_fs += 1\n",
    "                else:\n",
    "                    final_frames.append([])\n",
    "                    final_zs.append([])\n",
    "            final_frames_list.append(final_frames)\n",
    "            self.final_z_outputs.append(final_zs)\n",
    "\n",
    "        final_frames_list = face_enhancement(final_frames_list, self.model)\n",
    "        \n",
    "        if is_tgt_video:\n",
    "            assert False, \"not implemented\"\n",
    "        else:\n",
    "            result = get_final_image(final_frames_list, crop_frames_list, self.full_frames[0], self.tfm_array_list, self.handler)\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(\"\", (str, list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaiseung_ghost_cuda114",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
