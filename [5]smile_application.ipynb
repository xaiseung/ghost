{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/kornia/augmentation/augmentation.py:1830: DeprecationWarning: GaussianBlur is no longer maintained and will be removed from the future versions. Please use RandomGaussianBlur instead.\n",
      "  warnings.warn(\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:143: DeprecationWarning: In accordance with NEP 32, the function mirr was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  mirr = onp.mirr\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:160: DeprecationWarning: In accordance with NEP 32, the function npv was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  npv = onp.npv\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:164: DeprecationWarning: In accordance with NEP 32, the function pmt was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  pmt = onp.pmt\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:173: DeprecationWarning: In accordance with NEP 32, the function ppmt was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  ppmt = onp.ppmt\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:176: DeprecationWarning: In accordance with NEP 32, the function pv was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  pv = onp.pv\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy/fallback.py:177: DeprecationWarning: In accordance with NEP 32, the function rate was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
      "  rate = onp.rate\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy_dispatch_protocol.py:48: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  cur_np_ver = LooseVersion(_np.__version__)\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy_dispatch_protocol.py:49: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  np_1_17_ver = LooseVersion('1.17')\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy_dispatch_protocol.py:68: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  cur_np_ver = LooseVersion(_np.__version__)\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/mxnet/numpy_dispatch_protocol.py:69: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  np_1_15_ver = LooseVersion('1.15')\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "def nop(it, *a, **k):\n",
    "    return it\n",
    "\n",
    "real_tqdm = tqdm.tqdm\n",
    "tqdm.tqdm = nop\n",
    "\n",
    "import scipy\n",
    "scipy.sparse.csr.csr_matrix = scipy.sparse.csr_matrix\n",
    "\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "import cv2\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.inference.image_processing import crop_face, get_final_image, show_images, normalize_and_torch, normalize_and_torch_batch\n",
    "from utils.inference.video_processing import read_video, get_target, get_final_video, add_audio_from_another_video, face_enhancement, crop_frames_and_get_transforms, resize_frames\n",
    "from utils.inference.core import model_inference, transform_target_to_torch\n",
    "from utils.inference.faceshifter_run import faceshifter_batch, faceshifter_batch_zattrs\n",
    "from network.AEI_Net import AEI_Net\n",
    "from coordinate_reg.image_infer import Handler\n",
    "from insightface_func.face_detect_crop_multi import Face_detect_crop\n",
    "from arcface_model.iresnet import iresnet100\n",
    "from models.pix2pix_model import Pix2PixModel\n",
    "from models.config_sr import TestOptions\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinglePCA:\n",
    "    def __init__(self, layer_id, root_path=\"./pca_pkl\", use_norm=False):\n",
    "        self.layer_id = layer_id\n",
    "        self.use_norm = use_norm # -1~1\n",
    "        with open(f\"{root_path}/Altered_zattr{self.layer_id}PCA.pkl\", \"rb\") as file:\n",
    "            self.pca = pickle.load(file)\n",
    "        with open(f\"{root_path}/Altered_zattr{self.layer_id}PCAMinMax.pkl\", \"rb\") as file:\n",
    "            minmax = pickle.load(file)\n",
    "        self.pca_min = minmax[\"min\"]\n",
    "        self.pca_max = minmax[\"max\"]\n",
    "    \n",
    "    def transform(self, z_embed:np.ndarray) -> np.ndarray:\n",
    "        return self.pca.transform(z_embed)\n",
    "    \n",
    "    def inverse_transform(self, p_emb:np.ndarray) -> np.ndarray:\n",
    "        return self.pca.inverse_transform(p_emb)\n",
    "\n",
    "    def calcul_z_embed_diff(self, target:np.ndarray, original:np.ndarray)->np.ndarray:\n",
    "        target = self.inverse_transform(target)\n",
    "        original = self.inverse_transform(original)\n",
    "        return target-original\n",
    "    \n",
    "    def inject_pca_param(self, orig_z_embeds: List[np.ndarray], pca_param:dict, add=False)->List[np.ndarray]:\n",
    "        p_emb = self.transform(orig_z_embeds[self.layer_id])\n",
    "        orig_p_emb = p_emb.copy()\n",
    "        if self.use_norm:\n",
    "            for k in pca_param:\n",
    "                if add:\n",
    "                    p_emb[:, k] += pca_param[k] * (self.pca_max[k]-self.pca_min[k])/2\n",
    "                else:\n",
    "                    p_emb[:, k] = self.pca_min[k] + (pca_param[k]+1)/2*(self.pca_max[k]-self.pca_min[k])\n",
    "        else:\n",
    "            for k in pca_param:\n",
    "                p_emb[:, k] = pca_param[k]+p_emb[:, k]*int(add)\n",
    "        res_z_embeds = copy.deepcopy(orig_z_embeds)\n",
    "        res_z_embeds[self.layer_id] += self.calcul_z_embed_diff(p_emb, orig_p_emb)\n",
    "        return res_z_embeds\n",
    "\n",
    "class DoublePCA:\n",
    "    def __init__(self, layer_num=8, root_path=\"./pca_pkl\", use_norm=False):\n",
    "        self.layer_num = layer_num\n",
    "        self.use_norm = use_norm # -1~1\n",
    "\n",
    "        self.pca1_list = []\n",
    "        for z_i in range(self.layer_num):\n",
    "            with open(f\"{root_path}/Altered_zattr{z_i}PCA.pkl\", \"rb\") as file:\n",
    "                self.pca1_list.append(pickle.load(file))\n",
    "        with open(f\"{root_path}/Altered_zattr_doublePCA.pkl\", \"rb\") as file:\n",
    "            self.pca2 = pickle.load(file)\n",
    "        with open(f\"{root_path}/Altered_zattr_doublePCAMinMax.pkl\", \"rb\") as file:\n",
    "            minmax = pickle.load(file)\n",
    "        self.pca2_min = minmax[\"min\"]\n",
    "        self.pca2_max = minmax[\"max\"]\n",
    "    def transform(self, z_embeds) -> np.ndarray:\n",
    "        p1emb_array = [self.pca1_list[z_i].transform(z_embeds[z_i]) for z_i in range(self.layer_num)]\n",
    "        p1emb_array = np.concatenate(p1emb_array, axis=1)\n",
    "        return self.pca2.transform(p1emb_array)\n",
    "\n",
    "    def inverse_transform(self, p2emb_array)->List[np.ndarray]:\n",
    "        p1emb_array = self.pca2.inverse_transform(p2emb_array).reshape([-1, self.layer_num, 128])\n",
    "        z_embeds = [self.pca1_list[z_i].inverse_transform(p1emb_array[:,z_i]) for z_i in range(self.layer_num)]\n",
    "        return z_embeds\n",
    "\n",
    "    def calcul_z_embed_diff(self, target:np.ndarray, original:np.ndarray)->List[np.ndarray]:\n",
    "        assert target.shape[0] == original.shape[0]\n",
    "        target = self.inverse_transform(target)\n",
    "        original = self.inverse_transform(original)\n",
    "        return [target[i]-original[i] for i in range(self.layer_num)]\n",
    "\n",
    "    def inject_pca_param(self, orig_z_embeds: List[np.ndarray], pca_param:dict, add=False)->List[np.ndarray]:\n",
    "        p2emb = self.transform(orig_z_embeds)\n",
    "        orig_p2embed = p2emb.copy()\n",
    "        if self.use_norm:\n",
    "            for k in pca_param:\n",
    "                if add:\n",
    "                    p2emb[:, k] += pca_param[k] * (self.pca_max[k]-self.pca_min[k])/2\n",
    "                else:\n",
    "                    p2emb[:, k] = self.pca_min[k] + (pca_param[k]+1)/2*(self.pca_max[k]-self.pca_min[k])\n",
    "        else:\n",
    "            for k in pca_param:\n",
    "                p2emb[:, k] = pca_param[k]+p2emb[:, k]*int(add)\n",
    "        diff = self.calcul_z_embed_diff(p2emb, orig_p2embed)\n",
    "        return [orig_z_embeds[i]+diff[i] for i in range(len(orig_z_embeds))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FaceSwap_PCAInjection():\n",
    "\n",
    "    def __init__(self, pca_mode=\"double\",pca_use_norm=False):\n",
    "        self.crop_size = 224 \n",
    "\n",
    "        self.app = Face_detect_crop(name='antelope', root='./insightface_func/models')\n",
    "        self.app.prepare(ctx_id= 0, det_thresh=0.6, det_size=(640,640))\n",
    "\n",
    "        # main model for generation\n",
    "        self.G = AEI_Net(backbone='unet', num_blocks=2, c_id=512)\n",
    "        self.G.eval()\n",
    "        self.G.load_state_dict(torch.load('weights/G_unet_2blocks.pth', map_location=torch.device('cpu')))\n",
    "        self.G = self.G.cuda()\n",
    "        self.G = self.G.half()\n",
    "\n",
    "        # arcface model to get face embedding\n",
    "        self.netArc = iresnet100(fp16=False)\n",
    "        self.netArc.load_state_dict(torch.load('arcface_model/backbone.pth'))\n",
    "        self.netArc = self.netArc.cuda()\n",
    "        self.netArc.eval()\n",
    "\n",
    "        # model to get face landmarks\n",
    "        self.handler = Handler('./coordinate_reg/model/2d106det', 0, ctx_id=0, det_size=640)\n",
    "\n",
    "        # model to make superres of face, set use_sr=True if you want to use super resolution or use_sr=False if you don't\n",
    "        opt = TestOptions()\n",
    "        #opt.which_epoch ='10_7'\n",
    "        self.model = Pix2PixModel(opt)\n",
    "        self.model.netG.train()\n",
    "        if pca_mode == \"double\":\n",
    "            self.pca = DoublePCA(use_norm=pca_use_norm)\n",
    "        else:\n",
    "            try:\n",
    "                pca_mode = int(pca_mode)\n",
    "                assert pca_mode >= 0 and pca_mode <= 7\n",
    "                self.pca = SinglePCA(pca_mode, use_norm=pca_use_norm)\n",
    "            except:\n",
    "                raise ValueError(\"pca_mode should be 'double' or integer in [0~7]\")\n",
    "\n",
    "        \n",
    "    \n",
    "    def swap_face(self, source: Union[np.ndarray, str],\n",
    "                    target: Union[np.ndarray, str],\n",
    "                    is_tgt_video=False,\n",
    "                    BS = 60,\n",
    "                    pca_param: dict = None,\n",
    "                    pca_add: bool = False):\n",
    "        \"\"\" # TODO\n",
    "\n",
    "        source와 target은 cv2.imread의 출력, 즉 bgr ndarray 입력으로 간주한다.\n",
    "        단, target이 영상인 경우에는 [t, H, W, C]꼴의 ndarray 리스트 입력으로 생각한다\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if isinstance(source, str):\n",
    "            source = cv2.imread(source)\n",
    "        source = [crop_face(source, self.app, self.crop_size)[0][:,:,::-1]]\n",
    "\n",
    "        if isinstance(target, str):\n",
    "            if is_tgt_video:\n",
    "                full_frames, fps = read_video(tgt)\n",
    "            else:\n",
    "                target_full = cv2.imread(target)\n",
    "                full_frames = [target_full]\n",
    "        else:\n",
    "            full_frames = target if is_tgt_video else [target]\n",
    "\n",
    "        cropped_target = get_target(full_frames, self.app, self.crop_size)\n",
    "        target_norm = normalize_and_torch_batch(np.array(cropped_target))\n",
    "        target_embeds = self.netArc(F.interpolate(target_norm, scale_factor=0.5, mode='bilinear', align_corners=True))\n",
    "        crop_frames_list, tfm_array_list = crop_frames_and_get_transforms(full_frames,\n",
    "                                                                    target_embeds,\n",
    "                                                                    self.app,\n",
    "                                                                    self.netArc,\n",
    "                                                                    self.crop_size,\n",
    "                                                                    set_target=False,\n",
    "                                                                    similarity_th=0.15\n",
    "                                                                    )\n",
    "        crop_frames_list = crop_frames_list\n",
    "        tfm_array_list = tfm_array_list\n",
    "\n",
    "        source_embeds = []\n",
    "        for source_curr in source:\n",
    "            source_curr = normalize_and_torch(source_curr)\n",
    "            source_embeds.append(self.netArc(F.interpolate(source_curr, scale_factor=0.5, mode='bilinear', align_corners=True)))\n",
    "\n",
    "        final_frames_list = []\n",
    "        for idx, (crop_frames, tfm_array, source_embed) in enumerate(zip(crop_frames_list, tfm_array_list, source_embeds)):\n",
    "            # Resize croped frames and get vector which shows on which frames there were faces\n",
    "            resized_frs, present = resize_frames(crop_frames)\n",
    "            resized_frs = np.array(resized_frs)\n",
    "\n",
    "            # transform embeds of Xs and target frames to use by model\n",
    "            target_batch_rs = transform_target_to_torch(resized_frs, half=True)\n",
    "            #assert False\n",
    "            source_embed = source_embed.half()\n",
    "\n",
    "            # run model\n",
    "            size = target_batch_rs.shape[0]\n",
    "            model_output = []\n",
    "            for i in range(0, size, BS):\n",
    "                zattrs = self.G.get_attr(target_batch_rs[i:i+BS])\n",
    "                if pca_param:\n",
    "                    orig_z_shape = [zattrs[j].shape for j in range(len(zattrs))]\n",
    "                    np_zattrs = [zattrs[j].detach().cpu().numpy().reshape([orig_z_shape[j][0], -1]) for j in range(len(zattrs))]\n",
    "                    injected_z = self.pca.inject_pca_param(np_zattrs, pca_param, add=pca_add)\n",
    "                    zattrs = [torch.from_numpy(injected_z[j].reshape(*orig_z_shape[j])).half().cuda() for j in range(len(orig_z_shape))]\n",
    "                Y_st = faceshifter_batch_zattrs(source_embed, zattrs, BS, self.G)\n",
    "                model_output.append(Y_st)\n",
    "            torch.cuda.empty_cache()\n",
    "            model_output = np.concatenate(model_output)\n",
    "\n",
    "            # create list of final frames with transformed faces\n",
    "            final_frames = []\n",
    "            idx_fs = 0\n",
    "            for pres in present:\n",
    "                if pres == 1:\n",
    "                    final_frames.append(model_output[idx_fs])\n",
    "                    idx_fs += 1\n",
    "                else:\n",
    "                    final_frames.append([])\n",
    "            final_frames_list.append(final_frames)\n",
    "\n",
    "        final_frames_list = face_enhancement(final_frames_list, self.model)\n",
    "        \n",
    "        if is_tgt_video:\n",
    "            assert False, \"not implemented\"\n",
    "        else:\n",
    "            result = get_final_image(final_frames_list, crop_frames_list, full_frames[0], tfm_array_list, self.handler)\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input mean and std: 127.5 127.5\n",
      "find model: ./insightface_func/models/antelope/glintr100.onnx recognition\n",
      "find model: ./insightface_func/models/antelope/scrfd_10g_bnkps.onnx detection\n",
      "set det-size: (640, 640)\n",
      "loading ./coordinate_reg/model/2d106det 0\n",
      "input mean and std: 127.5 127.5\n",
      "find model: ./insightface_func/models/antelope/glintr100.onnx recognition\n",
      "find model: ./insightface_func/models/antelope/scrfd_10g_bnkps.onnx detection\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01:20:04] ../src/nnvm/legacy_json_util.cc:208: Loading symbol saved by previous version v1.5.0. Attempting to upgrade...\n",
      "[01:20:04] ../src/nnvm/legacy_json_util.cc:216: Symbol successfully upgraded!\n",
      "[01:20:04] ../src/base.cc:79: cuDNN lib mismatch: linked-against version 8204 != compiled-against version 8101.  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network [LIPSPADEGenerator] was created. Total number of parameters: 72.2 million. To see the architecture, do print(network).\n",
      "Load checkpoint from path:  weights/10_net_G.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/ipywidgets/widgets/widget.py:503: DeprecationWarning: Passing unrecognized arguments to super(FileUpload).__init__(aceept='').\n",
      "object.__init__() takes exactly one argument (the instance to initialize)\n",
      "This is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.\n",
      "  super().__init__(**kwargs)\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/ipywidgets/widgets/widget.py:503: DeprecationWarning: Passing unrecognized arguments to super(FileUpload).__init__(aceept='').\n",
      "object.__init__() takes exactly one argument (the instance to initialize)\n",
      "This is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.\n",
      "  super().__init__(**kwargs)\n",
      "/compuworks/anaconda3/envs/xaiseung_ghost_cuda114/lib/python3.9/site-packages/ipywidgets/widgets/interaction.py:43: DeprecationWarning: `ipykernel.pylab.backend_inline` is deprecated, directly use `matplotlib_inline.backend_inline`\n",
      "  from ipykernel.pylab.backend_inline import flush_figures\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b747709f1323452f94ed8d5691d90532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FileUpload(value=(), description='Source Upload'), FileUpload(value=(), description='Target Upl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4f4776a7a044cc97f53d32db2f22c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "faceswap = FaceSwap_PCAInjection(pca_mode=5,pca_use_norm=True)\n",
    "\n",
    "source_upload = widgets.FileUpload(\n",
    "    description=\"Source Upload\",\n",
    "    aceept=\"\",\n",
    "    multiple=False\n",
    ")\n",
    "target_upload = widgets.FileUpload(\n",
    "    description=\"Target Upload\",\n",
    "    aceept=\"\",\n",
    "    multiple=False\n",
    ")\n",
    "smile_button = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description=\"Make you Smile ;)\",\n",
    "    disabled=False,\n",
    "    icon=\"\"\n",
    ")\n",
    "\n",
    "line1_multibox = widgets.HBox(\n",
    "    children=[source_upload,\n",
    "    target_upload,\n",
    "    smile_button\n",
    "    ]\n",
    ")\n",
    "\n",
    "def plotting(source_value, target_value, smile: bool):\n",
    "    if bool(source_value) and bool(target_value):\n",
    "        source = cv2.imdecode(np.asarray(source_value[0][\"content\"], dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "        target = cv2.imdecode(np.asarray(target_value[0][\"content\"], dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "        plt.figure(num=1, clear=True, figsize=[12+int(smile)*4, 4])\n",
    "        plt.subplot(1, 3+int(smile), 1)\n",
    "        plt.imshow(source[:, :, ::-1])\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(1, 3+int(smile), 2)\n",
    "        plt.imshow(target[:, :, ::-1])\n",
    "        plt.axis(\"off\")\n",
    "        result = faceswap.swap_face(\n",
    "            source=source,\n",
    "            target=target)\n",
    "        plt.subplot(1, 3+int(smile), 3)\n",
    "        plt.imshow(result[:, :, ::-1])\n",
    "        plt.axis(\"off\")\n",
    "        if smile:\n",
    "            plt.subplot(1, 3+int(smile), 4)\n",
    "            result = faceswap.swap_face(\n",
    "            source=source,\n",
    "            target=target,\n",
    "            pca_param={8:1.5})\n",
    "            plt.imshow(result[:, :, ::-1])\n",
    "            plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "\n",
    "iplot = widgets.interactive_output(plotting,\n",
    "                                   {'source_value':source_upload,\n",
    "                                    'target_value':target_upload,\n",
    "                                    'smile':smile_button\n",
    "                                    })\n",
    "\n",
    "display(line1_multibox, iplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구 사용 예제 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input mean and std: 127.5 127.5\n",
      "find model: ./insightface_func/models/antelope/glintr100.onnx recognition\n",
      "find model: ./insightface_func/models/antelope/scrfd_10g_bnkps.onnx detection\n",
      "set det-size: (640, 640)\n",
      "loading ./coordinate_reg/model/2d106det 0\n",
      "input mean and std: 127.5 127.5\n",
      "find model: ./insightface_func/models/antelope/glintr100.onnx recognition\n",
      "find model: ./insightface_func/models/antelope/scrfd_10g_bnkps.onnx detection\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:06:48] ../src/nnvm/legacy_json_util.cc:208: Loading symbol saved by previous version v1.5.0. Attempting to upgrade...\n",
      "[10:06:48] ../src/nnvm/legacy_json_util.cc:216: Symbol successfully upgraded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network [LIPSPADEGenerator] was created. Total number of parameters: 72.2 million. To see the architecture, do print(network).\n",
      "Load checkpoint from path:  weights/10_net_G.pth\n"
     ]
    }
   ],
   "source": [
    "faceswap = FaceSwap_PCAInjection(pca_mode=5,pca_use_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no injection\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'faceswap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno injection\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m [\n",
      "\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexamples/images/jaeseung_3.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexamples/images/great-faker.webp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexamples/images/tgt2.png\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m      6\u001b[0m     ]:\n",
      "\u001b[0;32m----> 8\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfaceswap\u001b[49m\u001b[38;5;241m.\u001b[39mswap_face(\n",
      "\u001b[1;32m      9\u001b[0m         source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexamples/images/elon_musk.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m     10\u001b[0m         target\u001b[38;5;241m=\u001b[39mtarget)\n",
      "\u001b[1;32m     12\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(result[:, :, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;32m     13\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'faceswap' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"no injection\")\n",
    "for target in [\n",
    "    \"examples/images/jaeseung_3.jpg\",\n",
    "    \"examples/images/great-faker.webp\",\n",
    "    \"examples/images/tgt2.png\",\n",
    "    ]:\n",
    "\n",
    "    result = faceswap.swap_face(\n",
    "        source=\"examples/images/elon_musk.jpg\",\n",
    "        target=target)\n",
    "\n",
    "    plt.imshow(result[:, :, ::-1])\n",
    "    plt.show()\n",
    "print(\"=\"*30)\n",
    "print(\"=\"*30)\n",
    "print(\"After inject\")\n",
    "for target in [\n",
    "    \"examples/images/jaeseung_3.jpg\",\n",
    "    \"examples/images/great-faker.webp\",\n",
    "    \"examples/images/tgt2.png\",\n",
    "    ]:\n",
    "\n",
    "    result = faceswap.swap_face(\n",
    "        source=\"examples/images/elon_musk.jpg\",\n",
    "        target=target,\n",
    "        pca_param={\n",
    "            8: 1.5},\n",
    "        pca_add=True)\n",
    "\n",
    "    plt.imshow(result[:, :, ::-1])\n",
    "    plt.show()\n",
    "\n",
    "print(\"=\"*30)\n",
    "print(\"=\"*30)\n",
    "print(\"After inject\")\n",
    "for target in [\n",
    "    \"examples/images/jaeseung_3.jpg\",\n",
    "    \"examples/images/great-faker.webp\",\n",
    "    \"examples/images/tgt2.png\",\n",
    "    ]:\n",
    "\n",
    "    result = faceswap.swap_face(\n",
    "        source=\"examples/images/elon_musk.jpg\",\n",
    "        target=target,\n",
    "        pca_param={\n",
    "            8: 1.5},\n",
    "        pca_add=False)\n",
    "\n",
    "    plt.imshow(result[:, :, ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msource_upload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconetent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "source_upload.value[\"conetent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaiseung_ghost_cuda114",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
